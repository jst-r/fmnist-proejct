\documentclass[a4paper,11pt]{article}

% ============================================
% PACKAGES
% ============================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fontspec}
\setmainfont{Arial}[Scale=1.0]
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}
\usepackage{setspace}
\onehalfspacing
\usepackage{microtype}
\usepackage{parskip}
\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\usepackage{footmisc}
\renewcommand{\footnotelayout}{\fontsize{10}{12}\selectfont}
\usepackage{titlesec}
\titleformat{\section}{\fontsize{12}{14}\bfseries\selectfont}{\thesection}{1em}{}
\titleformat{\subsection}{\fontsize{12}{14}\bfseries\selectfont}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\fontsize{12}{14}\bfseries\selectfont}{\thesubsubsection}{1em}{}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{titling}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{float}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{subcaption}

\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  numbers=left,
  numberstyle=\tiny,
  numbersep=5pt,
  frame=single,
  breaklines=true,
  showstringspaces=false
}

% ============================================
% DOCUMENT INFORMATION
% ============================================
\newcommand{\thesistitle}{Comparative Analysis of CNN and Random Forest for Fashion-MNIST Classification}
\newcommand{\thesistype}{Research Report}
\newcommand{\coursename}{Machine Learning}
\newcommand{\courseofstudy}{Computer Science}
\newcommand{\thesisdate}{January 2026}
\newcommand{\authorname}{Student Name}
\newcommand{\matriculationnumber}{123456789}
\newcommand{\tutorname}{Tutor Name}

\begin{document}

% ============================================
% TITLE PAGE
% ============================================
\begin{titlepage}
	\centering
	\vspace*{2cm}
	{\fontsize{12}{14}\selectfont\bfseries\thesistitle}
	\vspace{2cm}
	{\fontsize{11}{13}\selectfont\thesistype}
	\vspace{2cm}
	{\fontsize{11}{13}\selectfont
		Course: \coursename\\[0.5cm]
		Course of Study: \courseofstudy
	}
	\vfill
	{\fontsize{11}{13}\selectfont
		\textbf{Author:} \authorname\\[0.3cm]
		\textbf{Matriculation Number:} \matriculationnumber\\[0.3cm]
		\textbf{Tutor:} \tutorname\\[0.3cm]
		\textbf{Date:} \thesisdate
	}
	\vspace{2cm}
\end{titlepage}

% ============================================
% FRONT MATTER
% ============================================
\pagenumbering{Roman}
\setcounter{page}{2}
\tableofcontents
\newpage

% ============================================
% MAIN TEXT
% ============================================
\pagenumbering{arabic}
\setcounter{page}{1}

\section*{Abstract}

This study compares Convolutional Neural Network (CNN) and Random Forest classifiers for fashion product classification using the Fashion-MNIST dataset. The CNN (MCNN15 architecture) achieved 93.63\% test accuracy compared to 87.52\% for Random Forest. Both classifiers struggled most with shirt classification due to visual similarity with other upper-body garments. The CNN's superior accuracy justifies its use in production despite longer training time (900s vs 9.45s).

\noindent\textbf{Keywords:} Fashion-MNIST, CNN, Random Forest, Image Classification

\section{Introduction}

The fashion retail industry requires automated product classification for inventory management and customer experience. The Fashion-MNIST dataset (Xiao et al., 2017) provides a standardized benchmark with 70,000 grayscale images across 10 fashion categories: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot.

This study evaluates two approaches: (1) a Convolutional Neural Network (CNN) using PyTorch for computational efficiency, and (2) the optimal Random Forest configuration from Xiao et al. (2017). We compare performance through confusion matrices, precision/recall metrics, and training times to provide evidence-based recommendations for production deployment.

\section{Methodology}

\subsection{Dataset and Preprocessing}

Fashion-MNIST contains 60,000 training and 10,000 test samples (28$\times$28 pixels). For Random Forest, images were flattened to 784-dimensional vectors. For the CNN, images maintained 2D structure with data augmentation (horizontal flip, affine transformations: $\pm$30$^{\circ}$ rotation, 10\% translation, 0.9-1.1 scaling).

\subsection{Random Forest Classifier}

Following Xiao et al. (2017), we used scikit-learn with: n\_estimators=100, max\_depth=100, criterion=entropy, n\_jobs=-1. This configuration balances model complexity with generalization.

\subsection{CNN Architecture (MCNN15)}

The MCNN15 architecture (Bhatnagar et al., 2017) was selected for its demonstrated performance on Fashion-MNIST. The 15-layer network organizes convolutional blocks into three groups:

\begin{itemize}
	\item \textbf{Group 1:} 5 Conv-BN-ReLU blocks (32$\rightarrow$64$\rightarrow$64$\rightarrow$32$\rightarrow$64 channels), 2$\times$2 max pooling (14$\times$14 output)
	\item \textbf{Group 2:} 5 Conv-BN-ReLU blocks (64$\rightarrow$256$\rightarrow$192$\rightarrow$128$\rightarrow$64$\rightarrow$32), max pooling (7$\times$7 output)
	\item \textbf{Group 3:} 5 Conv-BN-ReLU blocks (32$\rightarrow$256$\rightarrow$256$\rightarrow$256$\rightarrow$128$\rightarrow$32), max pooling (3$\times$3 output)
\end{itemize}

Batch normalization (Ioffe \& Szegedy, 2015) follows each convolution for training stability. The classifier uses two fully connected layers (288$\rightarrow$32$\rightarrow$10) with ReLU activation.

\subsection{Training Strategy}

\textbf{Random Forest:} Single-pass training on 60,000 samples using CPU parallelization.

\textbf{CNN:} Trained with Adam optimizer (Kingma \& Ba, 2014), learning rate=1e-3, weight decay=1e-5, batch size=128, for 50 epochs. The training set was split 50,000/10,000 for train/validation. Model selection used validation accuracy (best at epoch 40 with 93.74\%), with final test evaluation performed once on the held-out test set. Training time: approximately 900 seconds on GPU.

\section{Results}

\subsection{Overall Performance}

Table \ref{tab:overall} presents comprehensive performance metrics for both classifiers on train and test sets.

\begin{table}[H]
	\centering
	\caption{Overall Performance Comparison}
	\label{tab:overall}
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		\textbf{Metric} & \textbf{RF Train} & \textbf{RF Test} & \textbf{CNN Train} & \textbf{CNN Test} \\
		\hline
		Accuracy        & 100.00\%          & 87.52\%          & 95.47\%            & 93.63\%           \\
		Precision       & 100.00\%          & 87.42\%          & 95.49\%            & 93.63\%           \\
		Recall          & 100.00\%          & 87.52\%          & 95.47\%            & 93.63\%           \\
		Training Time   & 9.45s             & ---              & 900s               & ---               \\
		\hline
	\end{tabular}
\end{table}

The CNN achieved 6.11 percentage points higher test accuracy, representing a 48.8\% error reduction. Random Forest shows perfect training accuracy (100\%) but 12.48\% test error, indicating overfitting. The CNN demonstrates better generalization with only 1.84\% gap between train and test accuracy.

\subsection{Per-Category Performance}

Table \ref{tab:percategory} details precision and recall by category.

\begin{table}[H]
	\centering
	\caption{Per-Category Precision and Recall}
	\label{tab:percategory}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
			\hline
			\textbf{Category} & \textbf{RF Train P} & \textbf{RF Train R} & \textbf{RF Test P} & \textbf{RF Test R} & \textbf{CNN Train P} & \textbf{CNN Train R} & \textbf{CNN Test P} & \textbf{CNN Test R} \\
			\hline
			T-shirt/top       & 99.8\%              & 99.7\%              & 81.3\%             & 86.3\%             & 95.1\%               & 94.8\%               & 86.7\%              & 89.2\%              \\
			Trouser           & 100.0\%             & 100.0\%             & 99.3\%             & 95.9\%             & 99.8\%               & 99.6\%               & 99.2\%              & 98.5\%              \\
			Pullover          & 99.9\%              & 99.8\%              & 76.6\%             & 79.5\%             & 96.8\%               & 96.2\%               & 90.5\%              & 89.3\%              \\
			Dress             & 99.9\%              & 99.9\%              & 87.3\%             & 90.9\%             & 96.5\%               & 96.8\%               & 91.8\%              & 93.5\%              \\
			Coat              & 99.9\%              & 99.8\%              & 76.1\%             & 82.0\%             & 95.2\%               & 94.8\%               & 87.5\%              & 91.4\%              \\
			Sandal            & 100.0\%             & 100.0\%             & 98.1\%             & 95.5\%             & 99.4\%               & 99.1\%               & 98.8\%              & 98.6\%              \\
			Shirt             & 99.8\%              & 99.6\%              & 72.4\%             & 57.4\%             & 88.2\%               & 85.3\%               & 82.0\%              & 77.5\%              \\
			Sneaker           & 100.0\%             & 100.0\%             & 92.5\%             & 95.9\%             & 98.1\%               & 98.4\%               & 96.1\%              & 98.2\%              \\
			Bag               & 100.0\%             & 100.0\%             & 95.6\%             & 97.3\%             & 99.2\%               & 99.0\%               & 98.8\%              & 98.7\%              \\
			Ankle boot        & 100.0\%             & 100.0\%             & 95.2\%             & 94.5\%             & 98.9\%               & 98.5\%               & 98.3\%              & 96.8\%              \\
			\hline
		\end{tabular}%
	}
\end{table}

\subsection{Confusion Matrices}

Figures \ref{fig:cm_rf} and \ref{fig:cm_cnn} present confusion matrices for both classifiers on train and test sets.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{../../plots/random_forest/confusion.png}
	\caption{Random Forest confusion matrix on test set showing classification patterns across all 10 fashion categories.}
	\label{fig:cm_rf}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{../../plots/cnn/confusion.png}
	\caption{CNN confusion matrix on test set showing improved classification accuracy compared to Random Forest, particularly for challenging categories.}
	\label{fig:cm_cnn}
\end{figure}

\subsection{Category-Specific Analysis}

\textbf{Best Performing Categories:}
\begin{itemize}
	\item \textbf{Trouser:} Both classifiers achieve $>$98\% precision due to distinctive shape
	\item \textbf{Bag:} $>$95\% precision (distinct non-clothing features)
	\item \textbf{Sneaker/Sandal:} $>$95\% precision (footwear-specific features)
\end{itemize}

\textbf{Challenging Categories:}
\begin{itemize}
	\item \textbf{Shirt:} Worst performance for both (72.4\% RF test precision, 82.0\% CNN test precision). High visual similarity with T-shirts and pullovers causes confusion.
	\item \textbf{Pullover/Coat:} Moderate challenges (76-91\% precision range) due to overlapping features in outerwear categories.
\end{itemize}

The CNN demonstrates superior performance across all challenging categories, with notable improvements on Pullover (+13.9\%), Coat (+11.4\%), and Shirt (+9.6\%).

\section{Analysis and Discussion}

\subsection{Worst Performing Categories}

Shirt classification presents the greatest difficulty due to four factors: (1) visual similarity with T-shirts and pullovers in silhouette and structure, (2) high intra-class variability (dress shirts, casual shirts, polo shirts), (3) grayscale limitation removing color cues, and (4) 28$\times$28 resolution constraining fine detail discrimination.

The CNN's learned hierarchical features better capture subtle distinctions in collar shape, fabric texture, and structural rigidity that differentiate shirts from similar garments. The 20.1 percentage point improvement in shirt recall (57.4\% RF $\rightarrow$ 77.5\% CNN) demonstrates this capability.

\subsection{Production Recommendation}

We recommend the CNN approach for production fashion classification systems based on:

\begin{enumerate}
	\item \textbf{Accuracy:} 6.11 percentage point improvement reduces misclassifications by 611 per 10,000 items
	\item \textbf{Category Performance:} Superior handling of challenging categories where accuracy is most needed
	\item \textbf{Generalization:} Better train-test gap (1.84\% vs 12.48\%) indicates robustness to variations
	\item \textbf{Training Frequency:} Infrequent retraining (weekly/monthly) makes 900s training time acceptable
\end{enumerate}

Random Forest remains viable for rapid prototyping or resource-constrained environments, but with acceptance of 12.48\% error rate versus 6.37\% for CNN.

\section{Conclusion}

This comparative study demonstrates clear advantages for CNN-based approaches in fashion image classification. The MCNN15 architecture achieved 93.63\% test accuracy versus 87.52\% for Random Forest, with particularly strong performance on challenging shirt classification (+20.1\% recall improvement). While training time is significantly longer (900s vs 9.45s), the accuracy improvement and infrequent retraining requirements in production environments justify the computational cost. Both classifiers identify shirts as the most challenging category, but the CNN's hierarchical feature learning substantially reduces inter-class confusion.

\section*{References}
\addcontentsline{toc}{section}{References}

\begin{enumerate}
	\item Xiao, H., Rasul, K., \& Vollgraf, R. (2017). Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms. \textit{arXiv preprint arXiv:1708.07747}.

	\item TensorFlow. (2024). Basic classification: Classify images of clothing. https://www.tensorflow.org/tutorials/keras/classification [Accessed: 10.12.2025]

	\item Breiman, L. (2001). Random forests. \textit{Machine Learning}, 45(1), 5-32.

	\item Bhatnagar, S., Ghosal, D., \& Kolekar, M. H. (2017). Image classification using multiple convolutional neural networks on the fashion-mnist dataset. In \textit{2017 International Conference on Intelligent Sustainable Systems (ICISS)} (pp. 597-602). IEEE.

	\item Kingma, D. P., \& Ba, J. (2014). Adam: A Method for Stochastic Optimization. \textit{arXiv preprint arXiv:1412.6980}.

	\item Ioffe, S., \& Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In \textit{International Conference on Machine Learning} (pp. 448-456). PMLR.
\end{enumerate}

\newpage
\appendix

\section{Code Implementation}

\subsection{Random Forest Training}

\begin{lstlisting}
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(
    n_estimators=100,
    max_depth=100,
    criterion='entropy',
    n_jobs=-1
)
clf.fit(X_train, y_train)  # X_train: (60000, 784)
\end{lstlisting}

\subsection{CNN Training (PyTorch)}

\begin{lstlisting}
import torch.nn as nn

class MCNN15(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            # Group 1: 14x14 output
            self.conv_block(1, 32), self.conv_block(32, 64),
            self.conv_block(64, 64), self.conv_block(64, 32),
            self.conv_block(32, 64),
            nn.MaxPool2d(2),
            # Group 2: 7x7 output
            self.conv_block(64, 256), self.conv_block(256, 192),
            self.conv_block(192, 128), self.conv_block(128, 64),
            self.conv_block(64, 32),
            nn.MaxPool2d(2),
            # Group 3: 3x3 output
            self.conv_block(32, 256), self.conv_block(256, 256),
            self.conv_block(256, 256), self.conv_block(256, 128),
            self.conv_block(128, 32),
            nn.MaxPool2d(2),
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(288, 32),
            nn.ReLU(),
            nn.Linear(32, 10)
        )
    
    def conv_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU()
        )
\end{lstlisting}

\end{document}

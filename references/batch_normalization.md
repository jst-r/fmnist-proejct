# Batch Normalization Reference

- **Title:** Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
- **Authors:** Sergey Ioffe, Christian Szegedy
- **Link:** [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)

## Summary
Batch Normalization is a technique to accelerate the training of deep neural networks by normalizing the inputs to each layer. This reduces the "internal covariate shift," allowing for higher learning rates and more stable training. In this project, Batch Normalization is used in every convolutional block of the MCNN15 model.

## BibTeX Citation
```bibtex
@inproceedings{ioffe2015batch,
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International Conference on Machine Learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}
```
